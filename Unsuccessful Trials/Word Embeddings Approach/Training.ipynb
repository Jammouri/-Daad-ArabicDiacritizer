{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5fQa2_ZvcUW"
      },
      "source": [
        "# 1 - Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWtEozgzvcUc",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import pickle as pkl\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Embedding, Dense, Dropout, LSTM, Bidirectional, TimeDistributed, Conv1D, Lambda, Concatenate, MultiHeadAttention, LayerNormalization, SpatialDropout1D, Add\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.initializers import GlorotNormal\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bv82XUZBvcUp"
      },
      "source": [
        "# 2 - Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MeZD5sgEvcUs",
        "tags": []
      },
      "outputs": [],
      "source": [
        "WITH_EXTRA_TRAIN = False\n",
        "DATASET_PATH = 'Data'\n",
        "CONSTANTS_PATH = 'helpers/constants'\n",
        "\n",
        "with open(CONSTANTS_PATH + '/ARABIC_LETTERS_LIST.pickle', 'rb') as file:\n",
        "    ARABIC_LETTERS_LIST = pkl.load(file)\n",
        "with open(CONSTANTS_PATH + '/DIACRITICS_LIST.pickle', 'rb') as file:\n",
        "    DIACRITICS_LIST = pkl.load(file)\n",
        "if not WITH_EXTRA_TRAIN:\n",
        "    with open(CONSTANTS_PATH + '/RNN_BIG_CHARACTERS_MAPPING.pickle', 'rb') as file:\n",
        "        CHARACTERS_MAPPING = pkl.load(file)\n",
        "else:\n",
        "    with open(CONSTANTS_PATH + '/RNN_BIG_CHARACTERS_MAPPING.pickle', 'rb') as file:\n",
        "        CHARACTERS_MAPPING = pkl.load(file)\n",
        "with open(CONSTANTS_PATH + '/RNN_CLASSES_MAPPING.pickle', 'rb') as file:\n",
        "    CLASSES_MAPPING = pkl.load(file)\n",
        "with open(CONSTANTS_PATH + '/RNN_REV_CLASSES_MAPPING.pickle', 'rb') as file:\n",
        "    REV_CLASSES_MAPPING = pkl.load(file)\n",
        "\n",
        "REV_CHARACTERS_MAPPING = {value: key for key, value in CHARACTERS_MAPPING.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7bUFCjzvcUz"
      },
      "source": [
        "# 3 - Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3V92E3IjkCu",
        "tags": []
      },
      "outputs": [],
      "source": [
        "batch_size = 50\n",
        "train_raw = []\n",
        "\n",
        "for i in range(1, 280, batch_size):\n",
        "    batch_files = []\n",
        "    for j in range(i, min(i + batch_size, 280)):  # Ensure we don't exceed the total file count\n",
        "        filename = f\"/tashkeela_train/tashkeela_train_{j:03}.txt\"\n",
        "        with open(DATASET_PATH + filename, 'r', encoding='utf-8') as file:\n",
        "            lines = file.readlines()\n",
        "            batch_files.extend(lines)\n",
        "    train_raw.extend(batch_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBOXJI-EvcU1",
        "tags": []
      },
      "outputs": [],
      "source": [
        "val_raw = []\n",
        "\n",
        "for i in range(1, 15):\n",
        "    filename = f\"/tashkeela_val/tashkeela_val_{i:03}.txt\"\n",
        "    with open(DATASET_PATH + filename, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "        val_raw.extend(lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58eJStPAjkCu",
        "outputId": "2a52d84d-34a2-4f85-9015-dec92dc2e6a1",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2790000\n",
            "140000\n"
          ]
        }
      ],
      "source": [
        "print(len(train_raw))\n",
        "print(len(val_raw))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1_8v7CQFwi7"
      },
      "source": [
        "# 4 - Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyRf8eaKvcU_",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def remove_diacritics(data_raw):\n",
        "    return data_raw.translate(str.maketrans('', '', ''.join(DIACRITICS_LIST)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8AjA29fvcVL",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def to_one_hot(data, size):\n",
        "    one_hot = list()\n",
        "    for elem in data:\n",
        "        cur = [0] * size\n",
        "        cur[elem] = 1\n",
        "        one_hot.append(cur)\n",
        "    return one_hot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HJ4qihwvcU9"
      },
      "source": [
        "# 5 - Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSAkWSmTjkCw"
      },
      "outputs": [],
      "source": [
        "max_seq_len = 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54pOjNwtvcVi",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def split_data(data_raw):\n",
        "    data_new = list()\n",
        "\n",
        "    for line in data_raw:\n",
        "        for sub_line in line.split('\\n'):\n",
        "            if len(remove_diacritics(sub_line).strip()) == 0:\n",
        "                continue\n",
        "\n",
        "            if len(remove_diacritics(sub_line).strip()) > 0 and len(remove_diacritics(sub_line).strip()) <= max_seq_len:\n",
        "                data_new.append(sub_line.strip())\n",
        "            else:\n",
        "                sub_line = sub_line.split()\n",
        "                tmp_line = ''\n",
        "                for word in sub_line:\n",
        "                    if len(remove_diacritics(tmp_line).strip()) + len(remove_diacritics(word).strip()) + 1 > max_seq_len:\n",
        "                        if len(remove_diacritics(tmp_line).strip()) > 0:\n",
        "                            data_new.append(tmp_line.strip())\n",
        "                        tmp_line = word\n",
        "                    else:\n",
        "                        if tmp_line == '':\n",
        "                            tmp_line = word\n",
        "                        else:\n",
        "                            tmp_line += ' '\n",
        "                            tmp_line += word\n",
        "                if len(remove_diacritics(tmp_line).strip()) > 0:\n",
        "                    data_new.append(tmp_line.strip())\n",
        "\n",
        "    return data_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6tIgNkMvcVy",
        "tags": []
      },
      "outputs": [],
      "source": [
        "train_split = split_data(train_raw)\n",
        "val_split = split_data(val_raw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKBLfJvevcV8",
        "outputId": "56b6cc89-7ac6-4760-e799-433a3b49d266",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training examples (split): 2847305\n",
            "Validation examples (split): 142865\n"
          ]
        }
      ],
      "source": [
        "print('Training examples (split):', len(train_split))\n",
        "print('Validation examples (split):', len(val_split))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44Zf0Dy5vcVn",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def map_data(data_raw):\n",
        "    X = list()\n",
        "    Y = list()\n",
        "\n",
        "    for line in data_raw:\n",
        "        x = [CHARACTERS_MAPPING['<SOS>']]\n",
        "        y = [CLASSES_MAPPING['<SOS>']]\n",
        "\n",
        "        for idx, char in enumerate(line):\n",
        "                if char in DIACRITICS_LIST:\n",
        "                    continue\n",
        "\n",
        "                # if char wasn't a diacritic add it to x\n",
        "                try:\n",
        "                    x.append(CHARACTERS_MAPPING[char])\n",
        "                except KeyError as e:\n",
        "                    print(f\"Error: Character '{char}' not found in CHARACTERS_MAPPING at index {idx} in line: {line}\")\n",
        "\n",
        "                # if char wasn't a diacritic and wasn't an arabic letter add '' to y (no diacritic)\n",
        "                if char not in ARABIC_LETTERS_LIST:\n",
        "                    y.append(CLASSES_MAPPING[''])\n",
        "                # if char was an arabic letter only.\n",
        "                else:\n",
        "                    char_diac = ''\n",
        "                    if idx + 1 < len(line) and line[idx + 1] in DIACRITICS_LIST:\n",
        "                        char_diac = line[idx + 1]\n",
        "                        if idx + 2 < len(line) and line[idx + 2] in DIACRITICS_LIST and char_diac + line[idx + 2] in CLASSES_MAPPING:\n",
        "                            char_diac += line[idx + 2]\n",
        "                        elif idx + 2 < len(line) and line[idx + 2] in DIACRITICS_LIST and line[idx + 2] + char_diac in CLASSES_MAPPING: # شدة فتحة = فتحة شدة\n",
        "                            char_diac = line[idx + 2] + char_diac\n",
        "                    y.append(CLASSES_MAPPING[char_diac])\n",
        "\n",
        "\n",
        "        assert(len(x) == len(y))\n",
        "\n",
        "        x.append(CHARACTERS_MAPPING['<EOS>'])\n",
        "        y.append(CLASSES_MAPPING['<EOS>'])\n",
        "\n",
        "        # Padding\n",
        "        pad_len = max_seq_len - len(x)\n",
        "        x.extend([CHARACTERS_MAPPING['<PAD>']] * pad_len)  # Pad with '<PAD>' token\n",
        "        y.extend([CLASSES_MAPPING['<PAD>']] * pad_len)  # Pad with '<PAD>' token\n",
        "\n",
        "        y = to_one_hot(y, len(CLASSES_MAPPING))\n",
        "\n",
        "        X.append(x)\n",
        "        Y.append(y)\n",
        "    return X, Y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zo2-V244vcWC"
      },
      "source": [
        "# 6 - Model Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWD_4xBRjkCy"
      },
      "outputs": [],
      "source": [
        "class DataGenerator(Sequence):\n",
        "    def __init__(self, lines, batch_size):\n",
        "        self.lines = lines\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.lines) / float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        lines = self.lines[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        X_batch, Y_batch = map_data(lines)\n",
        "        lines_no_diac = [remove_diacritics(line) for line in lines]\n",
        "\n",
        "        X_max_seq_len = max_seq_len\n",
        "        Y_max_seq_len = max_seq_len\n",
        "\n",
        "        # assert(X_max_seq_len == Y_max_seq_len)\n",
        "\n",
        "        X = list()\n",
        "        for x in X_batch:\n",
        "            x = list(x)\n",
        "            x = x[:X_max_seq_len]\n",
        "            x.extend([CHARACTERS_MAPPING['<PAD>']] * (X_max_seq_len - len(x)))\n",
        "            X.append(np.asarray(x))\n",
        "\n",
        "        Y_tmp = list()\n",
        "        for y in Y_batch:\n",
        "            y_new = list(y)\n",
        "            y_new = y_new[:Y_max_seq_len]\n",
        "            y_new.extend(to_one_hot([CLASSES_MAPPING['<PAD>']] * (Y_max_seq_len - len(y)), len(CLASSES_MAPPING)))\n",
        "            Y_tmp.append(np.asarray(y_new))\n",
        "        Y_batch = Y_tmp\n",
        "\n",
        "        X = np.asarray(X)\n",
        "        Y_batch = np.asarray(Y_batch)\n",
        "\n",
        "\n",
        "        return X, Y_batch, lines_no_diac"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhgEQdhejkCy"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "numpy_data = np.load('numpy_data_batch128_max200.npy', allow_pickle=True)\n",
        "training_generator = DataGenerator(train_split, batch_size) # Convert characters to indices and get the actual text\n",
        "embedding_dim = numpy_data[0][1].shape[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_839USNjkCz"
      },
      "outputs": [],
      "source": [
        "char_inputs = Input(shape=(max_seq_len,), name='char_inputs')\n",
        "word_embeddings_input = Input(shape=(max_seq_len, embedding_dim), name='word_embeddings')\n",
        "\n",
        "char_embeddings = Embedding(input_dim=len(CHARACTERS_MAPPING), output_dim=1024, embeddings_initializer=GlorotNormal(seed=961))(char_inputs)\n",
        "merged_embeddings = Concatenate()([char_embeddings, word_embeddings_input])\n",
        "\n",
        "dense1 = Dense(units=256, activation='relu', kernel_initializer=GlorotNormal(seed=961))(merged_embeddings)\n",
        "conv1 = Conv1D(filters=128, kernel_size=3, activation='relu', padding='same')(dense1)\n",
        "blstm1 = Bidirectional(LSTM(units=256, return_sequences=True, kernel_initializer=GlorotNormal(seed=961)))(conv1)\n",
        "dropout1 = Dropout(0.5)(blstm1)\n",
        "blstm2 = Bidirectional(LSTM(units=256, return_sequences=True, kernel_initializer=GlorotNormal(seed=961)))(dropout1)\n",
        "dropout2 = Dropout(0.5)(blstm2)\n",
        "dense2 = TimeDistributed(Dense(units=512, activation='relu', kernel_initializer=GlorotNormal(seed=961)))(dropout2)\n",
        "dense3 = TimeDistributed(Dense(units=512, activation='relu', kernel_initializer=GlorotNormal(seed=961)))(dense2)\n",
        "output = TimeDistributed(Dense(units=len(CLASSES_MAPPING), activation='softmax', kernel_initializer=GlorotNormal(seed=961)))(dense3)\n",
        "\n",
        "model = Model(inputs=[char_inputs, word_embeddings_input], outputs=output)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6uvo94LjkCz",
        "outputId": "c66e0309-af05-416f-f906-1bb62bd20bf8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8/8 [==============================] - 1s 70ms/step - loss: 0.0564\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0567\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0563\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0844\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 0.0604\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0441\n",
            "8/8 [==============================] - 1s 71ms/step - loss: 0.0548\n",
            "8/8 [==============================] - 1s 71ms/step - loss: 0.0602\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 0.0578\n",
            "8/8 [==============================] - 1s 76ms/step - loss: 0.0554\n",
            "8/8 [==============================] - 1s 79ms/step - loss: 0.0549\n",
            "8/8 [==============================] - 1s 78ms/step - loss: 0.0609\n",
            "8/8 [==============================] - 1s 74ms/step - loss: 0.0620\n",
            "8/8 [==============================] - 1s 77ms/step - loss: 0.0674\n",
            "8/8 [==============================] - 1s 72ms/step - loss: 0.0546\n",
            "8/8 [==============================] - 1s 74ms/step - loss: 0.0500\n",
            "8/8 [==============================] - 1s 73ms/step - loss: 0.0626\n",
            "8/8 [==============================] - 1s 72ms/step - loss: 0.0687\n",
            "8/8 [==============================] - 1s 72ms/step - loss: 0.0582\n",
            "8/8 [==============================] - 1s 74ms/step - loss: 0.0565\n",
            "8/8 [==============================] - 1s 73ms/step - loss: 0.0547\n",
            "8/8 [==============================] - 1s 75ms/step - loss: 0.0563\n",
            "8/8 [==============================] - 1s 76ms/step - loss: 0.0506\n",
            "8/8 [==============================] - 1s 72ms/step - loss: 0.0709\n",
            "8/8 [==============================] - 1s 77ms/step - loss: 0.0399\n",
            "8/8 [==============================] - 1s 72ms/step - loss: 0.0521\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 0.0550\n",
            "8/8 [==============================] - 1s 72ms/step - loss: 0.0465\n",
            "8/8 [==============================] - 1s 73ms/step - loss: 0.0551\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 0.0549\n",
            "8/8 [==============================] - 1s 75ms/step - loss: 0.0574\n",
            "8/8 [==============================] - 1s 75ms/step - loss: 0.0593\n",
            "8/8 [==============================] - 1s 73ms/step - loss: 0.0538\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 0.0516\n",
            "8/8 [==============================] - 1s 74ms/step - loss: 0.0519\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 0.0603\n",
            "8/8 [==============================] - 1s 72ms/step - loss: 0.0565\n",
            "8/8 [==============================] - 1s 75ms/step - loss: 0.0485\n",
            "8/8 [==============================] - 1s 71ms/step - loss: 0.0535\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 0.0574\n",
            "8/8 [==============================] - 1s 71ms/step - loss: 0.0505\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 0.0539\n",
            "8/8 [==============================] - 1s 72ms/step - loss: 0.0502\n",
            "8/8 [==============================] - 1s 80ms/step - loss: 0.0573\n",
            "8/8 [==============================] - 1s 73ms/step - loss: 0.0544\n",
            "8/8 [==============================] - 1s 72ms/step - loss: 0.0443\n",
            "8/8 [==============================] - 1s 72ms/step - loss: 0.0468\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 0.0508\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 0.0479\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 0.0541\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 0.0482\n",
            "8/8 [==============================] - 1s 71ms/step - loss: 0.0501\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 0.0448\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 0.0559\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 0.0556\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 0.0511\n",
            "8/8 [==============================] - 1s 72ms/step - loss: 0.0543\n",
            "8/8 [==============================] - 1s 77ms/step - loss: 0.0509\n",
            "8/8 [==============================] - 1s 71ms/step - loss: 0.0535\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 0.0503\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0485\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0407\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 0.0472\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0462\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 0.0497\n",
            "8/8 [==============================] - 1s 72ms/step - loss: 0.0461\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 0.0402\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0485\n",
            "8/8 [==============================] - 1s 71ms/step - loss: 0.0580\n",
            "8/8 [==============================] - 1s 72ms/step - loss: 0.0487\n",
            "8/8 [==============================] - 1s 74ms/step - loss: 0.0493\n",
            "8/8 [==============================] - 1s 75ms/step - loss: 0.0497\n",
            "8/8 [==============================] - 1s 79ms/step - loss: 0.0454\n",
            "8/8 [==============================] - 1s 73ms/step - loss: 0.0475\n",
            "8/8 [==============================] - 1s 78ms/step - loss: 0.0540\n",
            "8/8 [==============================] - 1s 71ms/step - loss: 0.0510\n",
            "8/8 [==============================] - 1s 72ms/step - loss: 0.0488\n",
            "8/8 [==============================] - 1s 71ms/step - loss: 0.0461\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 0.0454\n",
            "6/6 [==============================] - 0s 69ms/step - loss: 0.0560\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 0.0480\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 0.0475\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0483\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0736\n",
            "8/8 [==============================] - 1s 71ms/step - loss: 0.0507\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0378\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 0.0495\n",
            "8/8 [==============================] - 1s 71ms/step - loss: 0.0508\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 0.0499\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 0.0482\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 0.0474\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0544\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 0.0515\n",
            "8/8 [==============================] - 1s 71ms/step - loss: 0.0552\n",
            "8/8 [==============================] - 1s 71ms/step - loss: 0.0497\n",
            "8/8 [==============================] - 1s 72ms/step - loss: 0.0432\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 0.0529\n",
            "8/8 [==============================] - 1s 72ms/step - loss: 0.0598\n",
            "8/8 [==============================] - 1s 71ms/step - loss: 0.0502\n",
            "8/8 [==============================] - 1s 71ms/step - loss: 0.0472\n",
            "8/8 [==============================] - 1s 72ms/step - loss: 0.0492\n",
            "8/8 [==============================] - 1s 72ms/step - loss: 0.0521\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 0.0431\n",
            "8/8 [==============================] - 1s 71ms/step - loss: 0.0609\n",
            "8/8 [==============================] - 1s 71ms/step - loss: 0.0355\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 0.0442\n",
            "8/8 [==============================] - 1s 72ms/step - loss: 0.0485\n",
            "8/8 [==============================] - 1s 71ms/step - loss: 0.0420\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 0.0465\n",
            "8/8 [==============================] - 1s 74ms/step - loss: 0.0477\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 0.0528\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 0.0523\n",
            "8/8 [==============================] - 1s 75ms/step - loss: 0.0458\n",
            "8/8 [==============================] - 1s 71ms/step - loss: 0.0437\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 0.0474\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 0.0549\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 0.0500\n",
            "8/8 [==============================] - 1s 71ms/step - loss: 0.0424\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 0.0455\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 0.0517\n",
            "8/8 [==============================] - 1s 78ms/step - loss: 0.0433\n",
            "8/8 [==============================] - 1s 81ms/step - loss: 0.0461\n",
            "8/8 [==============================] - 1s 71ms/step - loss: 0.0429\n",
            "8/8 [==============================] - 1s 78ms/step - loss: 0.0473\n",
            "8/8 [==============================] - 1s 71ms/step - loss: 0.0474\n",
            "8/8 [==============================] - 1s 71ms/step - loss: 0.0386\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 0.0406\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0471\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0432\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0455\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0428\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 0.0445\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 0.0391\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0471\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0480\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 0.0441\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0497\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0454\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0485\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0468\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0453\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0379\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 0.0425\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 0.0395\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0429\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0428\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 0.0347\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0409\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0516\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 0.0458\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0446\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 0.0441\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0401\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0418\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0485\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0460\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0443\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0417\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 0.0405\n",
            "6/6 [==============================] - 0s 66ms/step - loss: 0.0510\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 0.0420\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0446\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0452\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 0.0661\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0472\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0339\n",
            "8/8 [==============================] - 1s 72ms/step - loss: 0.0428\n",
            "8/8 [==============================] - 1s 74ms/step - loss: 0.0473\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 0.0449\n",
            "8/8 [==============================] - 1s 71ms/step - loss: 0.0435\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 0.0413\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 0.0477\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 0.0489\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0495\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0424\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0387\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 0.0472\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 0.0537\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0468\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 0.0435\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 0.0425\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0463\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 0.0395\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 0.0562\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0311\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 0.0418\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0428\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 0.0371\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 0.0418\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 0.0424\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 0.0451\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0467\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 0.0428\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0414\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 0.0402\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 0.0494\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0454\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0390\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0423\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0441\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0383\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 0.0452\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0403\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 0.0428\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 0.0420\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 0.0370\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0356\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 0.0403\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0406\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0438\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 0.0390\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0413\n",
            "8/8 [==============================] - 1s 73ms/step - loss: 0.0353\n",
            "8/8 [==============================] - 1s 73ms/step - loss: 0.0452\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0444\n",
            "8/8 [==============================] - 1s 74ms/step - loss: 0.0392\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 0.0442\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0384\n",
            "8/8 [==============================] - 1s 71ms/step - loss: 0.0420\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 0.0413\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0393\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0346\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0395\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0356\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0393\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 0.0367\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0301\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 0.0371\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 0.0462\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0383\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0402\n",
            "8/8 [==============================] - 1s 71ms/step - loss: 0.0423\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0362\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0365\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 0.0441\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 0.0411\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0395\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 0.0395\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0373\n",
            "6/6 [==============================] - 0s 65ms/step - loss: 0.0440\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "# model.fit([char_input_data, aligned_word_embeddings], training_generator[0][1], epochs=10, batch_size=1)\n",
        "# I want to create a loop to train the model on all the batches\n",
        "for j in range(3):\n",
        "    for i in range(len(training_generator)):\n",
        "        char_input_data = np.load(f\"Embeddings/char_input_data_{i}.npy\")\n",
        "        aligned_word_embeddings = np.load(f\"Embeddings/aligned_word_embeddings_{i}.npy\")\n",
        "        target_data = training_generator[i][1]\n",
        "        model.fit([char_input_data, aligned_word_embeddings], target_data, epochs=1, batch_size=16)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7UGza4sjkCz"
      },
      "outputs": [],
      "source": [
        "# save the model\n",
        "model.save('modelembedd.h5')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
