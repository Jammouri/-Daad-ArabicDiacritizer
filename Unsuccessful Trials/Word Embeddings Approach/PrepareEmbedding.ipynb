{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from tensorflow.keras.utils import Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WITH_EXTRA_TRAIN = False\n",
    "DATASET_PATH = 'Data'\n",
    "CONSTANTS_PATH = 'helpers/constants'\n",
    "\n",
    "with open(CONSTANTS_PATH + '/ARABIC_LETTERS_LIST.pickle', 'rb') as file:\n",
    "    ARABIC_LETTERS_LIST = pkl.load(file)\n",
    "with open(CONSTANTS_PATH + '/DIACRITICS_LIST.pickle', 'rb') as file:\n",
    "    DIACRITICS_LIST = pkl.load(file)\n",
    "if not WITH_EXTRA_TRAIN:\n",
    "    with open(CONSTANTS_PATH + '/RNN_BIG_CHARACTERS_MAPPING.pickle', 'rb') as file:\n",
    "        CHARACTERS_MAPPING = pkl.load(file)\n",
    "else:\n",
    "    with open(CONSTANTS_PATH + '/RNN_BIG_CHARACTERS_MAPPING.pickle', 'rb') as file:\n",
    "        CHARACTERS_MAPPING = pkl.load(file)\n",
    "with open(CONSTANTS_PATH + '/RNN_CLASSES_MAPPING.pickle', 'rb') as file:\n",
    "    CLASSES_MAPPING = pkl.load(file)\n",
    "with open(CONSTANTS_PATH + '/RNN_REV_CLASSES_MAPPING.pickle', 'rb') as file:\n",
    "    REV_CLASSES_MAPPING = pkl.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = 'Data'\n",
    "\n",
    "train_data_raw = []\n",
    "valid_data_raw = []\n",
    "test_data_raw = []\n",
    "\n",
    "for i in range(1, 2):\n",
    "    filename = f\"/tashkeela_train/tashkeela_train_{i:03}.txt\"\n",
    "    with open(DATASET_PATH + filename, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        train_data_raw.extend(lines)\n",
    "\n",
    "\n",
    "for i in range(1, 2):\n",
    "    filename = f\"/tashkeela_val/tashkeela_val_{i:03}.txt\"\n",
    "    with open(DATASET_PATH + filename, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        valid_data_raw.extend(lines)\n",
    "\n",
    "for i in range(1, 2):\n",
    "    filename = f\"/tashkeela_test/tashkeela_test_{i:03}.txt\"\n",
    "    with open(DATASET_PATH + filename, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        test_data_raw.extend(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_diacritics(data_raw):\n",
    "    return data_raw.translate(str.maketrans('', '', ''.join(DIACRITICS_LIST)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(data, size):\n",
    "    one_hot = list()\n",
    "    for elem in data:\n",
    "        cur = [0] * size\n",
    "        cur[elem] = 1\n",
    "        one_hot.append(cur)\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data_raw):\n",
    "    data_new = list()\n",
    "\n",
    "    for line in data_raw:\n",
    "        for sub_line in line.split('\\n'):\n",
    "            if len(remove_diacritics(sub_line).strip()) == 0:\n",
    "                continue\n",
    "\n",
    "            if len(remove_diacritics(sub_line).strip()) > 0 and len(remove_diacritics(sub_line).strip()) <= max_seq_len:\n",
    "                data_new.append(sub_line.strip())\n",
    "            else:\n",
    "                sub_line = sub_line.split()\n",
    "                tmp_line = ''\n",
    "                for word in sub_line:\n",
    "                    if len(remove_diacritics(tmp_line).strip()) + len(remove_diacritics(word).strip()) + 1 > max_seq_len:\n",
    "                        if len(remove_diacritics(tmp_line).strip()) > 0:\n",
    "                            data_new.append(tmp_line.strip())\n",
    "                        tmp_line = word\n",
    "                    else:\n",
    "                        if tmp_line == '':\n",
    "                            tmp_line = word\n",
    "                        else:\n",
    "                            tmp_line += ' '\n",
    "                            tmp_line += word\n",
    "                if len(remove_diacritics(tmp_line).strip()) > 0:\n",
    "                    data_new.append(tmp_line.strip())\n",
    "\n",
    "    return data_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = split_data(train_data_raw)\n",
    "val_split = split_data(valid_data_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples (split): 10195\n",
      "Validation examples (split): 10188\n"
     ]
    }
   ],
   "source": [
    "print('Training examples (split):', len(train_split))\n",
    "print('Validation examples (split):', len(val_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My explanation for map_data function:\n",
    "# X is a mapped text without diacritics and with <SOS> and <EOS> tokens\n",
    "# Y is a one-hot encoded list of diacritics for each character in X\n",
    "# So, for each character in X, we have a corresponding diacritic in Y\n",
    "# Then X == Y\n",
    "\n",
    "def map_data(data_raw):\n",
    "    X = list()\n",
    "    Y = list()\n",
    "\n",
    "    for line in data_raw:\n",
    "        x = [CHARACTERS_MAPPING['<SOS>']]\n",
    "        y = [CLASSES_MAPPING['<SOS>']]\n",
    "\n",
    "        for idx, char in enumerate(line):\n",
    "                if char in DIACRITICS_LIST:\n",
    "                    continue\n",
    "\n",
    "                # if char wasn't a diacritic add it to x\n",
    "                try:\n",
    "                    x.append(CHARACTERS_MAPPING[char])\n",
    "                except KeyError as e:\n",
    "                    print(f\"Error: Character '{char}' not found in CHARACTERS_MAPPING at index {idx} in line: {line}\")\n",
    "\n",
    "                # if char wasn't a diacritic and wasn't an arabic letter add '' to y (no diacritic)\n",
    "                if char not in ARABIC_LETTERS_LIST:\n",
    "                    y.append(CLASSES_MAPPING[''])\n",
    "                # if char was an arabic letter only.\n",
    "                else:\n",
    "                    char_diac = ''\n",
    "                    if idx + 1 < len(line) and line[idx + 1] in DIACRITICS_LIST:\n",
    "                        char_diac = line[idx + 1]\n",
    "                        if idx + 2 < len(line) and line[idx + 2] in DIACRITICS_LIST and char_diac + line[idx + 2] in CLASSES_MAPPING:\n",
    "                            char_diac += line[idx + 2]\n",
    "                        elif idx + 2 < len(line) and line[idx + 2] in DIACRITICS_LIST and line[idx + 2] + char_diac in CLASSES_MAPPING: # شدة فتحة = فتحة شدة\n",
    "                            char_diac = line[idx + 2] + char_diac\n",
    "                    y.append(CLASSES_MAPPING[char_diac])\n",
    "\n",
    "        \n",
    "        assert(len(x) == len(y))\n",
    "\n",
    "        x.append(CHARACTERS_MAPPING['<EOS>'])\n",
    "        y.append(CLASSES_MAPPING['<EOS>'])\n",
    "\n",
    "        # Padding\n",
    "        pad_len = max_seq_len - len(x)\n",
    "        x.extend([CHARACTERS_MAPPING['<PAD>']] * pad_len)  # Pad with '<PAD>' token\n",
    "        y.extend([CLASSES_MAPPING['<PAD>']] * pad_len)  # Pad with '<PAD>' token\n",
    "\n",
    "        y = to_one_hot(y, len(CLASSES_MAPPING))\n",
    "\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "\n",
    "        # print(len(x))\n",
    "        # print(\"yyyyyyyyyyyyyyyyyyyyyy\")\n",
    "        # print(len(y))\n",
    "\n",
    "    # X = np.asarray(X)\n",
    "    # Y = np.asarray(Y)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load The Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\py310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\user\\anaconda3\\envs\\py310\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load the SentenceTransformer model\n",
    "embedding_model = SentenceTransformer('intfloat/multilingual-e5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_embeddings(sentences):\n",
    "    word_embeddings = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()  # Tokenize sentence into words\n",
    "        embeddings = embedding_model.encode(words)  # Generate word embeddings\n",
    "        word_embeddings.append((words, embeddings))\n",
    "    return word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_embeddings_with_characters(sentences, word_embeddings_data, max_seq_len, embedding_dim):\n",
    "    aligned_embeddings = np.zeros((len(sentences), max_seq_len, embedding_dim))\n",
    "    \n",
    "    for i, sentence in enumerate(sentences):\n",
    "        words, embeddings = word_embeddings_data[i]\n",
    "        char_pos = 0\n",
    "        for word, embedding in zip(words, embeddings):\n",
    "            word_len = len(word)\n",
    "            if char_pos + word_len > max_seq_len:\n",
    "                break  # Ensure we do not exceed max_seq_len\n",
    "            aligned_embeddings[i, char_pos:char_pos + word_len] = embedding\n",
    "            char_pos += word_len\n",
    "            if char_pos >= max_seq_len:\n",
    "                break\n",
    "    \n",
    "    return aligned_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, lines, batch_size):\n",
    "        self.lines = lines\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.lines) / float(self.batch_size)))\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lines = self.lines[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        X_batch, Y_batch = map_data(lines)\n",
    "        lines_no_diac = [remove_diacritics(line) for line in lines]\n",
    "\n",
    "        X_max_seq_len = max_seq_len\n",
    "        Y_max_seq_len = max_seq_len\n",
    "\n",
    "        # assert(X_max_seq_len == Y_max_seq_len)\n",
    "\n",
    "        X = list()\n",
    "        for x in X_batch:\n",
    "            x = list(x)\n",
    "            x = x[:X_max_seq_len]\n",
    "            x.extend([CHARACTERS_MAPPING['<PAD>']] * (X_max_seq_len - len(x)))\n",
    "            X.append(np.asarray(x))\n",
    "\n",
    "        Y_tmp = list()\n",
    "        for y in Y_batch:\n",
    "            y_new = list(y)\n",
    "            y_new = y_new[:Y_max_seq_len]\n",
    "            y_new.extend(to_one_hot([CLASSES_MAPPING['<PAD>']] * (Y_max_seq_len - len(y)), len(CLASSES_MAPPING)))\n",
    "            Y_tmp.append(np.asarray(y_new))\n",
    "        Y_batch = Y_tmp\n",
    "\n",
    "        X = np.asarray(X)\n",
    "        Y_batch = np.asarray(Y_batch)\n",
    "\n",
    "\n",
    "        return X, Y_batch, lines_no_diac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Start of creating the Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "training_generator = DataGenerator(train_split, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_input_data = training_generator[0][0] # Batch 0, indices vectors (batch_size, max_seq_len)\n",
    "word_embeddings_data = generate_word_embeddings(training_generator[0][2]) # For batch 0, (128 Sentences, (each Word with its Embeddings)) \n",
    "                                                                          # if in sentence3 7 words then word_embeddings_data[2][0] = 7 \n",
    "                                                                          # and word_embeddings_data[2][1] = 7x(Embedding Dim) Embeddings for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = word_embeddings_data[0][1].shape[1]\n",
    "aligned_word_embeddings = align_embeddings_with_characters(training_generator[0][2], word_embeddings_data, max_seq_len, embedding_dim) # This will return the following:\n",
    "                                                                                                                                       # (Batch_size, max_seq_len, Embedding_dim)\n",
    "                                                                                                                                       # (128, 200, 384)\n",
    "                                                                                                                                       # 200: each element is the embeddings of a word\n",
    "                                                                                                                                       # that related to a character in the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_input_data = np.array(char_input_data)\n",
    "aligned_word_embeddings = np.array(aligned_word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the generated data\n",
    "np.save('char_input_data.npy', char_input_data)\n",
    "np.save('aligned_word_embeddings.npy', aligned_word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's simulate the process of loading the data\n",
    "char_input_data = np.load('char_input_data.npy')\n",
    "aligned_word_embeddings = np.load('aligned_word_embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's simulate the process of generating the data\n",
    "training_generator = DataGenerator(train_split, batch_size)\n",
    "\n",
    "numpy_data = np.zeros((len(training_generator), 2), dtype=object)\n",
    "\n",
    "for batch in range(len(training_generator)): # For each batch\n",
    "    char_input_data = training_generator[batch][0] # Vector of indices (batch_size, max_seq_len)\n",
    "    word_embeddings_data = generate_word_embeddings(training_generator[batch][2])\n",
    "    embedding_dim = word_embeddings_data[0][1].shape[1]\n",
    "    aligned_word_embeddings = align_embeddings_with_characters(training_generator[batch][2], word_embeddings_data, max_seq_len, embedding_dim)\n",
    "\n",
    "    char_input_data = np.array(char_input_data)\n",
    "    aligned_word_embeddings = np.array(aligned_word_embeddings)\n",
    "\n",
    "    # Add to numpy_data\n",
    "    numpy_data[batch, 0] = char_input_data\n",
    "    numpy_data[batch, 1] = aligned_word_embeddings\n",
    "\n",
    "# Save the generated data\n",
    "np.save('numpy_data_batch128_max200.npy', numpy_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "numpy_data = np.load('numpy_data_batch128_max200.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_input_data = numpy_data[0][0]\n",
    "aligned_word_embeddings = numpy_data[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 200, 384)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
