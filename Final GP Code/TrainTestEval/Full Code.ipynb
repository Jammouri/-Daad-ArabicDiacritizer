{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sWtEozgzvcUc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding, Dense, Dropout, LSTM, Bidirectional, TimeDistributed, Conv1D, Lambda, Concatenate, MultiHeadAttention, LayerNormalization, SpatialDropout1D, Add\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.initializers import GlorotNormal\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- Load Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MeZD5sgEvcUs",
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = '../Data'\n",
    "CONSTANTS_PATH = '../helpers/constants'\n",
    "\n",
    "with open(CONSTANTS_PATH + '/ARABIC_LETTERS_LIST.pickle', 'rb') as file:\n",
    "    ARABIC_LETTERS_LIST = pkl.load(file)\n",
    "with open(CONSTANTS_PATH + '/DIACRITICS_LIST.pickle', 'rb') as file:\n",
    "    DIACRITICS_LIST = pkl.load(file)\n",
    "with open(CONSTANTS_PATH + '/RNN_BIG_CHARACTERS_MAPPING.pickle', 'rb') as file:\n",
    "    CHARACTERS_MAPPING = pkl.load(file)\n",
    "with open(CONSTANTS_PATH + '/RNN_CLASSES_MAPPING.pickle', 'rb') as file:\n",
    "    CLASSES_MAPPING = pkl.load(file)\n",
    "with open(CONSTANTS_PATH + '/RNN_REV_CLASSES_MAPPING.pickle', 'rb') as file:\n",
    "    REV_CLASSES_MAPPING = pkl.load(file)\n",
    "REV_CHARACTERS_MAPPING = {value: key for key, value in CHARACTERS_MAPPING.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t7bUFCjzvcUz"
   },
   "source": [
    "# 3 - Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "Number_of_training_files = 280 # 279 Max\n",
    "train_raw = []\n",
    "\n",
    "for i in range(1, Number_of_training_files, batch_size): \n",
    "    batch_files = []\n",
    "    for j in range(i, min(i + batch_size, Number_of_training_files)):  # Ensure we don't exceed the total file count\n",
    "        filename = f\"/tashkeela_train/tashkeela_train_{j:03}.txt\"\n",
    "        with open(DATASET_PATH + filename, 'r', encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "            batch_files.extend(lines)\n",
    "    train_raw.extend(batch_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vBOXJI-EvcU1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_raw = []\n",
    "Number_of_validation_files = 16 # 15 Max\n",
    "\n",
    "for i in range(1, Number_of_validation_files):\n",
    "    filename = f\"/tashkeela_val/tashkeela_val_{i:03}.txt\"\n",
    "    with open(DATASET_PATH + filename, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        val_raw.extend(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_raw = []\n",
    "Number_of_test_files = 16 # 15 Max\n",
    "\n",
    "for i in range(1, Number_of_test_files):\n",
    "    filename = f\"/tashkeela_test/tashkeela_test_{i:03}.txt\"\n",
    "    with open(DATASET_PATH + filename, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        test_raw.extend(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2790000\n",
      "150000\n",
      "150000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_raw))\n",
    "print(len(val_raw))\n",
    "print(len(test_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I1_8v7CQFwi7"
   },
   "source": [
    "# 4 - Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HyRf8eaKvcU_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_diacritics(Diacritized_data):\n",
    "    \"\"\"\n",
    "    Remove diacritics from the data\n",
    "    This function receives a string and removes the diacritics from it\n",
    "    Input: Diacritized_data: String with diacritics\n",
    "    Output: String with no diacritics\n",
    "    \"\"\"\n",
    "    \n",
    "    for diacritic in DIACRITICS_LIST:\n",
    "        Diacritized_data = Diacritized_data.replace(diacritic, '')\n",
    "    return Diacritized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z8AjA29fvcVL",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_one_hot(data, size):\n",
    "    \"\"\"\n",
    "    Convert the data to one hot encoding\n",
    "    This function receives a list of integers and converts it to one hot encoding\n",
    "    Used for converting the labels to one hot encoding for model training\n",
    "    Input: data: List of integers\n",
    "    size: Size of the one hot encoding\n",
    "    Output: List of one hot encoded vectors\n",
    "    \"\"\"\n",
    "    \n",
    "    one_hot = list()\n",
    "    for elem in data:\n",
    "        cur = [0] * size\n",
    "        cur[elem] = 1\n",
    "        one_hot.append(cur)\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7HJ4qihwvcU9"
   },
   "source": [
    "# 5 - Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "54pOjNwtvcVi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_data(data_raw, max_seq_len=400):\n",
    "    \"\"\"\n",
    "    Split the data into sequences of length less than or equal to max_seq_len\n",
    "    This function receives a list of strings and splits them into sequences of length less than or equal to max_seq_len\n",
    "    Input: data_raw: List of strings\n",
    "    Output: List of strings with sequences of length less than or equal to max_seq_len\n",
    "    \"\"\"\n",
    "\n",
    "    data_new = list()\n",
    "\n",
    "    for line in data_raw:\n",
    "        for sub_line in line.split('\\n'):\n",
    "            if len(remove_diacritics(sub_line).strip()) == 0:\n",
    "                continue\n",
    "\n",
    "            if len(remove_diacritics(sub_line).strip()) > 0 and len(remove_diacritics(sub_line).strip()) <= max_seq_len:\n",
    "                data_new.append(sub_line.strip())\n",
    "            else:\n",
    "                sub_line = sub_line.split()\n",
    "                tmp_line = ''\n",
    "                for word in sub_line:\n",
    "                    if len(remove_diacritics(tmp_line).strip()) + len(remove_diacritics(word).strip()) + 1 > max_seq_len:\n",
    "                        if len(remove_diacritics(tmp_line).strip()) > 0:\n",
    "                            data_new.append(tmp_line.strip())\n",
    "                        tmp_line = word\n",
    "                    else:\n",
    "                        if tmp_line == '':\n",
    "                            tmp_line = word\n",
    "                        else:\n",
    "                            tmp_line += ' '\n",
    "                            tmp_line += word\n",
    "                if len(remove_diacritics(tmp_line).strip()) > 0:\n",
    "                    data_new.append(tmp_line.strip())\n",
    "\n",
    "    return data_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p6tIgNkMvcVy",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_split = split_data(train_raw, 400)\n",
    "val_split = split_data(val_raw, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pKBLfJvevcV8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples (split): 2795219\n",
      "Validation examples (split): 150270\n"
     ]
    }
   ],
   "source": [
    "print('Training examples (split):', len(train_split))\n",
    "print('Validation examples (split):', len(val_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "44Zf0Dy5vcVn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def map_data(data_raw, max_seq_len=400):\n",
    "    \"\"\"\n",
    "    Map the data to the required format for training\n",
    "    This function receives a list of strings and maps them to the required format for training\n",
    "    Input: data_raw: List of strings\n",
    "    Output: X: List of mapped strings without diacritics and with <SOS> and <EOS> tokens\n",
    "    Y: List of one hot encoded vectors for each character in X\n",
    "    \"\"\"\n",
    "\n",
    "    X = list()\n",
    "    Y = list()\n",
    "\n",
    "    for line in data_raw:\n",
    "        x = [CHARACTERS_MAPPING['<SOS>']]\n",
    "        y = [CLASSES_MAPPING['<SOS>']]\n",
    "\n",
    "        for idx, char in enumerate(line):\n",
    "                if char in DIACRITICS_LIST:\n",
    "                    continue\n",
    "                # if char wasn't a diacritic add it to x\n",
    "                try:\n",
    "                    x.append(CHARACTERS_MAPPING[char])\n",
    "                except KeyError as e:\n",
    "                    print(f\"Error: Character '{char}' not found in CHARACTERS_MAPPING at index {idx} in line: {line}\")\n",
    "\n",
    "                # if char wasn't a diacritic and wasn't an arabic letter add '' to y (no diacritic)\n",
    "                if char not in ARABIC_LETTERS_LIST:\n",
    "                    y.append(CLASSES_MAPPING[''])\n",
    "                # if char was an arabic letter only.\n",
    "                else:\n",
    "                    char_diac = ''\n",
    "                    if idx + 1 < len(line) and line[idx + 1] in DIACRITICS_LIST:\n",
    "                        char_diac = line[idx + 1]\n",
    "                        if idx + 2 < len(line) and line[idx + 2] in DIACRITICS_LIST and char_diac + line[idx + 2] in CLASSES_MAPPING:\n",
    "                            char_diac += line[idx + 2]\n",
    "                        elif idx + 2 < len(line) and line[idx + 2] in DIACRITICS_LIST and line[idx + 2] + char_diac in CLASSES_MAPPING: # شدة فتحة = فتحة شدة\n",
    "                            char_diac = line[idx + 2] + char_diac\n",
    "                    y.append(CLASSES_MAPPING[char_diac])\n",
    "\n",
    "        assert(len(x) == len(y))\n",
    "\n",
    "        \n",
    "\n",
    "        x.append(CHARACTERS_MAPPING['<EOS>'])\n",
    "        y.append(CLASSES_MAPPING['<EOS>'])\n",
    "\n",
    "        x = x[:max_seq_len]\n",
    "        y = y[:max_seq_len]\n",
    "\n",
    "        # Padding\n",
    "        pad_len = max_seq_len - len(x)\n",
    "        x.extend([CHARACTERS_MAPPING['<PAD>']] * pad_len)  # Pad with '<PAD>' token\n",
    "        y.extend([CLASSES_MAPPING['<PAD>']] * pad_len)  # Pad with '<PAD>' token\n",
    "\n",
    "        y = to_one_hot(y, len(CLASSES_MAPPING))\n",
    "\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    \"\"\"\n",
    "    Data generator class for manage the data and feed it to the model in batches\n",
    "    Attributes:\n",
    "    lines: List of strings\n",
    "    batch_size: Integer\n",
    "\n",
    "    Methods:\n",
    "    __len__: Returns the number of batches\n",
    "    __getitem__: Returns the batch at the given index\n",
    "\n",
    "    Output:\n",
    "    X: List of mapped strings without diacritics and with <SOS> and <EOS> tokens\n",
    "    Y: List of one hot encoded vectors for each character in X\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lines, batch_size):\n",
    "        self.lines = lines\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.lines) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lines = self.lines[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        X_batch, Y_batch = map_data(lines, max_seq_len)\n",
    "\n",
    "        X = np.asarray(X_batch)\n",
    "        Y = np.asarray(Y_batch)\n",
    "\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LbDRPPUqvcWE",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=(max_seq_len,))\n",
    "\n",
    "embeddings = Embedding(input_dim=len(CHARACTERS_MAPPING),\n",
    "                        output_dim=1024,\n",
    "                        embeddings_initializer=GlorotNormal(seed=961))(inputs)\n",
    "\n",
    "conv1 = Conv1D(filters=512, kernel_size=3, activation='relu', padding='same')(embeddings)\n",
    "norm1 = LayerNormalization()(conv1)\n",
    "\n",
    "\n",
    "# First BLSTM layer\n",
    "blstm1 = Bidirectional(LSTM(units=256, return_sequences=True, kernel_initializer=GlorotNormal(seed=961)))(norm1)\n",
    "dropout1 = Dropout(0.6)(blstm1)\n",
    "res1 = Add()([norm1, dropout1])\n",
    "norm2 = LayerNormalization()(res1)\n",
    "\n",
    "# Second BLSTM layer\n",
    "blstm2 = Bidirectional(LSTM(units=256, return_sequences=True, kernel_initializer=GlorotNormal(seed=961)))(norm2)\n",
    "dropout2 = Dropout(0.6)(blstm2)\n",
    "res2 = Add()([norm2, dropout2])\n",
    "\n",
    "\n",
    "# Dense layers\n",
    "dense2 = TimeDistributed(Dense(units=512, activation='relu', kernel_initializer=GlorotNormal(seed=961)))(res2)\n",
    "dense3 = TimeDistributed(Dense(units=512, activation='relu', kernel_initializer=GlorotNormal(seed=961)))(dense2)\n",
    "\n",
    "# Output layer\n",
    "output = TimeDistributed(Dense(units=len(CLASSES_MAPPING), activation='softmax', kernel_initializer=GlorotNormal(seed=961)))(dense3)\n",
    "\n",
    "model = Model(inputs, output)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 400)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 400, 1024)    93184       ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 400, 512)     1573376     ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 400, 512)    1024        ['conv1d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  (None, 400, 512)     1574912     ['layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 400, 512)     0           ['bidirectional[0][0]']          \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 400, 512)     0           ['layer_normalization[0][0]',    \n",
      "                                                                  'dropout[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 400, 512)    1024        ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirectional  (None, 400, 512)    1574912     ['layer_normalization_1[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 400, 512)     0           ['bidirectional_1[0][0]']        \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 400, 512)     0           ['layer_normalization_1[0][0]',  \n",
      "                                                                  'dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " time_distributed (TimeDistribu  (None, 400, 512)    262656      ['add_1[0][0]']                  \n",
      " ted)                                                                                             \n",
      "                                                                                                  \n",
      " time_distributed_1 (TimeDistri  (None, 400, 512)    262656      ['time_distributed[0][0]']       \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      " time_distributed_2 (TimeDistri  (None, 400, 19)     9747        ['time_distributed_1[0][0]']     \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,353,491\n",
      "Trainable params: 5,353,491\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TgXwLe2tvcWV"
   },
   "source": [
    "# 7 - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S7wms8ngvcWk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit_model(model, epochs, batch_size, train_split, val_split):\n",
    "    \"\"\"\n",
    "    Fit the model on the training data using the given number of epochs and batch size using DataGenerator object\n",
    "    \n",
    "    Input: model: Model\n",
    "    epochs: Integer\n",
    "    batch_size: Integer\n",
    "    train_split: List of strings\n",
    "    val_split: List of strings\n",
    "    \"\"\"\n",
    "\n",
    "    random.shuffle(train_split)\n",
    "    random.shuffle(val_split)\n",
    "\n",
    "    training_generator = DataGenerator(train_split, batch_size)\n",
    "    val_generator = DataGenerator(val_split, batch_size)\n",
    "\n",
    "    model.fit(training_generator, validation_data=val_generator, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model(model, 5, 128, train_split, val_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "By8aqORavcWz"
   },
   "source": [
    "# 8 - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XhlYxg3KvcWf"
   },
   "outputs": [],
   "source": [
    "def predict(line, model, max_seq_len):\n",
    "    \"\"\"\n",
    "    Predict the diacritics for the given line using the model\n",
    "                    \n",
    "    Input: line: String\n",
    "    model: Model\n",
    "    Output: String with diacritics\n",
    "    \"\"\"\n",
    "    \n",
    "    X, _ = map_data([line], max_seq_len)\n",
    "    predictions = model.predict(X).squeeze()\n",
    "    predictions = predictions[1:]\n",
    "\n",
    "    diacritized_line = ''\n",
    "    for idx, char in enumerate(line):\n",
    "        if char in ARABIC_LETTERS_LIST:\n",
    "            diacritized_line += char\n",
    "            max_idx = np.argmax(predictions[idx])\n",
    "            diacritized_line += REV_CLASSES_MAPPING[max_idx]\n",
    "        else:\n",
    "            diacritized_line += char\n",
    "    \n",
    "    return diacritized_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model .h5\n",
    "model = tf.keras.models.load_model('..\\models\\shape400withResiduals2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "95UNs3BVs1jg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 118ms/step\n"
     ]
    }
   ],
   "source": [
    "output = predict(\"إن الذي ملأ اللغات محاسنا ... جعل الجمال وسره في الضاد.\", model, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'إِنَّ الَّذِي مَلَأَ اللُّغَاتِ مَحَاسِنًا ... جَعَلَ الجَمَالَ وَسِرَّهُ فِي الضَّادِ.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "model.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
